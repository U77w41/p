[{"content":"Welcome to My Blog Hello and welcome to my blog! I\u0026rsquo;m Ujjwal Chowdhury, a data scientist and research analyst based in Kolkata, India. With a Masterâ€™s degree in Data Science and over a year of practical experience, I specialize in Natural Language Processing (NLP), Generative AI, and Stock Forecasting. My passion lies in designing and developing innovative solutions using Artificial Intelligence, implementing machine learning models, and collaborating with cross-functional teams to deliver impactful, data-driven insights.\nAreas of Expertise Natural Language Processing (NLP) Generative AI Machine Learning Algorithms Time Series Analysis Algorithm Optimization Statistical Analysis Data Visualization Model Development Cross-functional Collaboration Skills Data Visualization Microsoft Power BI Excel Tableau Seaborn Plotly Matplotlib Machine Learning and Deep Learning Feature Engineering Model Development Hyper-parameter Tuning Neural Networks Reinforcement Learning Transfer Learning Optimization Techniques MLOps Tools/Frameworks Python, R TensorFlow, PyTorch, Keras, TFLite, MLFlow, PySpark PostgreSQL Azure, AWS LangChain, streamlit, Docker, Pydantic Natural Language Processing Text Generation Sentiment Analysis Speech Recognition Named Entity Recognition Text Classification LLM Prompt Engineering Computer Vision Image Processing Object Detection Image Classification Image Segmentation Image Generation Data Analysis and Mining Data Mining Web Scraping Statistical Analysis Time Series Analysis Anomaly Detection Predictive Analytics Survival Analysis Soft Skills Problem-Solving Teamwork Active Listening Adaptability Communication Analytical Thinking Professional Experience Research Executive (AI \u0026amp; NLP) Feedsense AI Private Limited, Kolkata, India (Mar 2024 - Present)\nUtilized reinforcement learning models integrating financial data to predict market movements and develop optimized trading strategies. AI Researcher Vista Intelligence Private Limited, Kolkata, India (Jan 2023 - Mar 2024)\nLed the NLP team, overseeing project developments and team operations. Fine-tuned an RNN-Transducer driven speech-to-text model to effectively capture Indian accents, reducing the Word Error Rate from 56.8% to 23.4%. Developed a live audio transcription model for real-time news analysis. Created a trade signal generator model integrating live audio, textual news articles, OHLC data, and quantitative techniques with over 75% directional accuracy. Developed an auto question generator program for applicant CVs. Utilized OpenAI API with Langchain for a large document summarizer. Employed a 4-bit quantized Mistral 7b LLM model for summarizing conference call conversations. Research \u0026amp; Publication Investigate How Market Behaves: Toward an Explanatory Multitasking Based Analytical Model for Financial Investments\nIEEE Access, March 2024\nDOI: 10.1109/ACCESS.2024.3369033 Courses \u0026amp; Certifications Artificial Intelligence (AI) for Investments (April 2023) - NPTEL Cloud Computing and Distributed Systems (March 2023) - NPTEL NISM-Series-XV: Research Analyst (Feb. 2023) - National Institute of Securities Markets Database Management System (Oct. 2022) - NPTEL Deep Learning for Computer Vision (Oct. 2022) - NPTEL Data Science Math Skills (April 2020) - Duke University, Coursera Education MSc Data Science RKMVERI, Belur, West Bengal, India (2021-2023)\nBSc Mathematics Vidyasagar University, Medinipur, West Bengal, India (2017-2020)\nPersonal Projects Fin-Bot: Advanced Agent-based Financial Chatbot\nDomain: NLP, LLM, Generative AI, Deep Learning, RAG\nIntegrated web search functionality for comprehensive query responses. Implemented a custom vector database for efficient retrieval of financial news and transcripts. Employed LLM-equipped agent for directed user queries, ensuring comprehensive insights. Sales Forecasting and Anomaly Detection on Walmart Sales Dataset\nDomain: Machine Learning, Time Series Analysis, Deep Learning\nUsed Factor Analysis for feature extraction. Applied time series, machine learning, and deep learning for sales prediction. Used unsupervised techniques for anomaly detection. Deep Bidirectional LSTM Network for Textual Sentiment Analysis\nDomain: Deep Learning, Sentiment Analysis, NLP, Web Scraping\nIntegrated Twitter API for real-time tweet scraping. Used AsyncHTMLSession to scrape news articles from Google News. Leveraged Bi-LSTM architecture for sentiment classification. Brain Tumor Classification\nDomain: Computer Vision, Deep Learning, Optimization Techniques\nUsed Transfer Learning and fine-tuned several pre-trained models. Explored various optimization algorithms. Applied snapshot learning technique for ensemble predictive model construction. Statistical Analysis of Diet, Exercise, and Fitness\nDomain: EDA, Data Visualization, Data Analysis, Statistical Inference\nCollected data using online surveys. Employed descriptive statistics for dataset summarization. Used Power BI, Tableau, Excel, R, and Python for analysis and visualization. Stay tuned for more insights, projects, and discussions on data science, AI, and beyond. Feel free to connect with me on LinkedIn or explore my projects on GitHub.\nThank you for visiting my blog!\n","date":"2024-05-26T00:00:00Z","image":"https://u77w41.github.io/p/hello-world/hello_huaef4065a6d79ee027d5c25c7abf67182_178944_120x120_fill_box_smart1_3.png","permalink":"https://u77w41.github.io/p/hello-world/","title":"Hello World"},{"content":"**GitHub Repo\nForecasting Techniques: Regression Techniques Polynomial Regression: Leveraging the flexibility of polynomial functions to capture non-linear relationships in the data. Multiple Linear Regression: Utilizing multiple features to model the sales data and uncover complex dependencies. Lasso Linear Regression: Introducing L1 regularization to encourage sparsity in the model, selecting only the most influential features. Ridge Linear Regression: Applying L2 regularization to prevent overfitting and improve the generalization of the model. Elastic Net Regression: Combining L1 and L2 regularization to harness the strengths of both techniques. Time Series Forecasting ARMA (AutoRegressive Moving Average): Employing ARMA models to understand and predict the temporal patterns in Walmart sales data. Anomaly Detection Techniques: Anomaly Detection Techniques using Unsupervised Learning Approaches KNN Regressor: Utilizing the K-nearest neighbors algorithm to detect anomalies based on the deviation from the expected sales values. DBSCAN (Density-Based Spatial Clustering of Applications with Noise): Identifying anomalies by clustering the sales data and isolating points that do not conform to any cluster. LSTM with Autoencoders: Leveraging Long Short-Term Memory networks combined with autoencoders for capturing complex temporal dependencies and detecting anomalies in the sales data. Data Version Control (DVC) To maintain the integrity and traceability of our project, we adopted the Data Version Control (DVC) Python library. DVC facilitated the versioning of our data, ensuring reproducibility and providing a robust framework for collaboration.\nThis project represents a holistic exploration of machine learning techniques for forecasting and anomaly detection, combining traditional regression methods with cutting-edge time series forecasting and unsupervised learning approaches.\n","date":"2022-06-27T00:00:00Z","image":"https://u77w41.github.io/p/forcasting-and-anomaly-detection/forecasting_huc2d27e2363a16ffb156d326714b7b69a_157551_120x120_fill_box_smart1_3.png","permalink":"https://u77w41.github.io/p/forcasting-and-anomaly-detection/","title":"Forcasting and Anomaly Detection"},{"content":"Overview This project focuses on the classification of brain tumor images into four distinct classes: \u0026lsquo;glioma\u0026rsquo;, \u0026lsquo;meningioma\u0026rsquo;, \u0026rsquo;notumor\u0026rsquo;, and \u0026lsquo;pituitary\u0026rsquo;. The primary approach involves fine-tuning the VGG16 model using the PyTorch framework. Additionally, Python\u0026rsquo;s OpenCV library is utilized for image processing, and various image augmentation techniques are applied to enhance model generalization.\nGitHub Link\nTechnologies Used PyTorch: The deep learning framework for model development and training. OpenCV: Used for image processing tasks. Python: The primary programming language for the project. Model Architecture The VGG16 model is employed as the base architecture for the brain tumor classification task. The model is fine-tuned to adapt to the specific requirements of the project.\nImage Augmentation Techniques To enhance the model\u0026rsquo;s robustness and improve generalization, the following image augmentation techniques are applied:\nRotation Scaling Horizontal and Vertical Flipping Brightness and Contrast adjustments Optimization Techniques Several optimization techniques are experimented with to fine-tune the model:\nStochastic Gradient Descent (SGD): A traditional optimization algorithm. Adam: An adaptive learning rate optimization algorithm. RMSprop: Another adaptive learning rate optimization algorithm. AdaGrad: Adaptive gradient optimization algorithm. Experimentation and Tuning The optimization parameters for each technique are carefully tuned to achieve optimal performance. The model undergoes thorough experimentation to identify the best combination of hyperparameters and optimization algorithms for the given task.\nConclusion The Brain Tumour Classification project demonstrates the successful implementation of a fine-tuned VGG16 model for accurately classifying brain tumor images into four classes. The combination of image augmentation techniques and experimentation with various optimization algorithms contributes to the overall success of the project.\nFor detailed implementation code and results, refer to the project repository and notebooks.\nPhoto by Wikipedia\n","date":"2022-05-02T00:00:00Z","image":"https://u77w41.github.io/p/brain-tumour-classificaion/Anaplastic_astrocytoma_huafe560acc88d562fbfc31204ab597709_204715_120x120_fill_q75_box_smart1.jpg","permalink":"https://u77w41.github.io/p/brain-tumour-classificaion/","title":"Brain Tumour Classificaion"},{"content":"Github Repo\nObjectives Implement the Flajolet-Martin Algorithm in a scalable manner suitable for processing the extensive corpus of Swami Vivekananda\u0026rsquo;s works. Develop a robust data processing pipeline to handle the text data and generate the necessary input for the algorithm. Fine-tune the algorithm parameters and validate its accuracy against known cardinality benchmarks to ensure reliable estimates. Technologies and Tools Programming Language: Python Data Processing: Pandas, Spark Flajolet-Martin Algorithm Implementation: Custom Python code Version Control: Git Documentation: Markdown Expected Outcomes A scalable and efficient implementation of the Flajolet-Martin Algorithm tailored for the unique characteristics of Swami Vivekananda\u0026rsquo;s complete works. Accurate cardinality estimates for the distinct elements in the dataset. Documentation detailing the project methodology, implementation details, and findings. Future Work Potential future enhancements could include exploring other probabilistic algorithms for cardinality estimation, optimizing the algorithm further, or extending the analysis to specific subsets of Swami Vivekananda\u0026rsquo;s works.\n","date":"2023-12-08T00:00:00Z","image":"https://u77w41.github.io/p/analysis-on-complete-works-of-swami-vivekananda/swami-ji_hu159688c089f0913868e94e743467a7a0_252624_120x120_fill_box_smart1_3.png","permalink":"https://u77w41.github.io/p/analysis-on-complete-works-of-swami-vivekananda/","title":"Analysis on Complete Works of Swami Vivekananda"}]